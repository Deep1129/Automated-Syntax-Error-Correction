# -*- coding: utf-8 -*-
"""demo-eval.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AgEvn023F4-6gru88Kx4ntGU-q85i0Sw
"""

import pandas as pd
import numpy as np
import ast
import sys

df_test_path = 'drive/MyDrive/'+sys.argv[1]
store_output_path = 'drive/MyDrive/' + sys.argv[2]

df_test = pd.read_csv(df_test_path)

for i in range(df_test.shape[0]):
  df_test['sourceLineTokens'][i] = ast.literal_eval(df_test['sourceLineTokens'][i])
  if len(df_test['sourceLineTokens'][i])>60:
    df_test['sourceLineTokens'][i] = df_test['sourceLineTokens'][i][0:60]
  df_test['sourceLineTokens'][i].append("EOS_token")
  df_test['sourceLineTokens'][i].insert(0,"SOS_token")
  n=len(df_test['sourceLineTokens'][i])
  while(n<62):
    df_test['sourceLineTokens'][i].append("PAD_token")
    n+=1

for i in range(df_test.shape[0]):
  df_test['targetLineTokens'][i] = ast.literal_eval(df_test['targetLineTokens'][i])
  if len(df_test['targetLineTokens'][i])>60:
    df_test['targetLineTokens'][i] = df_test['targetLineTokens'][i][0:60]
  df_test['targetLineTokens'][i].append("EOS_token")
  df_test['targetLineTokens'][i].insert(0,"SOS_token")
  n=len(df_test['targetLineTokens'][i])
  while(n<62):
    df_test['targetLineTokens'][i].append("PAD_token")
    n+=1

"""**Load Dictionary**"""

f = open("drive/MyDrive/Vocabfile.txt", "r")

contents = f.read()
dictionary = ast.literal_eval(contents)

dict_index= dictionary

k=250

vectors = np.zeros((len(dict_index),k+1))              #array for every unique token

oov_vector = np.zeros((k+1))                           # for OOV tokens
oov_vector[k] =1

for i in range(k):
  vectors[i][i] =1
for i in range(k,len(dict_index)):
  vectors[i] = oov_vector

encoder_input_test=[]
for i in range(df_test.shape[0]):
  e=[]
  x = df_test['sourceLineTokens'][i]
  for j in x:
    if (j in dict_index):
      index = dict_index[j]
    else:
      index=250
    e.append(np.array(vectors[index]))
 
  encoder_input_test.append(np.array(e))

encoder_input_test = np.array(encoder_input_test)

input_texts_test = df_test['sourceLineTokens']

output_texts_test=df_test['targetLineTokens']

input_token_index ={}
i=0
for key,value in dict_index.items():
  if i<250:
    input_token_index[key] = i
  else:
    input_token_index['OOV_Token'] = 250
  i+=1

target_token_index ={}
i=0
for key,value in dict_index.items():
  if i<250:
    target_token_index[key] = i
  else:
    target_token_index['OOV_Token'] = 250
  i+=1

import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence



"""# DECODE MODEL

"""

batch_size = 64  # Batch size for training.
epochs = 60 # Number of epochs to train for.
latent_dim = 256  # Latent dimensionality of the encoding space.
num_samples = 14643
num_decoder_tokens=251
num_encoder_tokens=251

model = keras.models.load_model("drive/MyDrive/Model_cp")

max_len = 62
max_len_target =62

max_encoder_seq_length= max_len
num_encoder_tokens=k+1
max_decoder_seq_length= max_len_target
num_decoder_tokens=k+1

encoder_inputs = model.input[0]  # input_1
encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1
encoder_states = [state_h_enc, state_c_enc]
encoder_model = keras.Model(encoder_inputs, encoder_states)

decoder_inputs = model.input[1]  # input_2
decoder_state_input_h = keras.Input(shape=(latent_dim,), name="input_3")
decoder_state_input_c = keras.Input(shape=(latent_dim,), name="input_4_")
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_lstm = model.layers[3]
decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(
    decoder_inputs, initial_state=decoder_states_inputs
)
decoder_states = [state_h_dec, state_c_dec]
decoder_dense = model.layers[4]
decoder_outputs = decoder_dense(decoder_outputs)
decoder_model = keras.Model(
    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states
)

# Reverse-lookup token index to decode sequences back to
# something readable.
reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())
reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())


def decode_sequence(input_seq):
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq)

    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1, 1, num_decoder_tokens))
    # Populate the first character of target sequence with the start character.
    # target_seq[0, 0, target_token_index["SOS"]] = 1.0

    # Sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1).
    stop_condition = False
    decoded_sentence = ""
    decoded_list=[]
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_char_index[sampled_token_index]
        decoded_list.append(sampled_char)

        # Exit condition: either hit max length
        # or find stop character.
        if sampled_char == "EOS_token" or len(decoded_sentence) > max_len_target:
            stop_condition = True
        # if len(decoded_sentence) > max_len_target:
        #     stop_condition = True

        # Update the target sequence (of length 1).
        target_seq = np.zeros((1, 1, num_decoder_tokens))
        target_seq[0, 0, sampled_token_index] = 1.0

        # Update states
        states_value = [h, c]
    return decoded_list

decoded_list=[]
for i in range(df_test.shape[0]):
  input_seq = encoder_input_test[i:i+1]
  # print("Input sentence:", input_seq.shape)
  decoded_sentence = decode_sequence(input_seq)
  decoded_sentence.pop()

  decoded_list.append(str(decoded_sentence))

output_df = pd.read_csv(df_test_path)

output_df['fixedTokens'] = decoded_list

output_df.to_csv('drive/MyDrive/demo-output-csv-file.csv')